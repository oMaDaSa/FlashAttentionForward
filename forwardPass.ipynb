{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "import math\n",
    "\n",
    "@triton.jit\n",
    "def kernel(q_ptr, k_ptr, v_ptr, o_ptr,\n",
    "           head_qty:tl.constexpr, seq_len:tl.constexpr, head_size:tl.constexpr,\n",
    "           softmax_scale:tl.constexpr, num_blocks:tl.constexpr, block_size:tl.constexpr):\n",
    "    head_i = tl.program_id(0)\n",
    "    block_i = tl.program_id(1)\n",
    "\n",
    "    qo_cols = tl.arange(0, head_size)\n",
    "    qo_rows = block_size * block_i + tl.arange(0, block_size)[:, None]\n",
    "    qo_ofs = head_i * head_size * seq_len + qo_rows * head_size + qo_cols\n",
    "    qo_mask = (qo_rows < seq_len) & (qo_cols < head_size)\n",
    "\n",
    "    q_ofs = q_ptr + qo_ofs\n",
    "    q_block = tl.load(q_ofs, mask = qo_mask)\n",
    "\n",
    "    o_block = tl.zeros_like(q_block)\n",
    "    m_i = tl.zeros((block_size,), dtype = tl.float32) - float('inf')\n",
    "    l_i = tl.zeros((block_size,), dtype = tl.float32)\n",
    "\n",
    "    for block_j in range(0, num_blocks):\n",
    "      kv_cols = tl.arange(0, head_size)\n",
    "      kv_rows = block_size * block_j + tl.arange(0, block_size)[:, None]\n",
    "      kv_ofs = head_i * head_size * seq_len + kv_rows * head_size + kv_cols\n",
    "      kv_mask = (kv_rows < seq_len) & (kv_cols < head_size)\n",
    "\n",
    "      k_ofs = k_ptr + kv_ofs\n",
    "      k_block = tl.load(k_ofs, mask = kv_mask)\n",
    "\n",
    "      v_ofs = v_ptr + kv_ofs\n",
    "      v_block = tl.load(v_ofs, mask = kv_mask)\n",
    "\n",
    "      qk_block = tl.dot(q_block, k_block.T, allow_tf32 = False) * softmax_scale\n",
    "\n",
    "      m_ij = tl.maximum(m_i, tl.max(qk_block, 1))\n",
    "\n",
    "      qk_block -= m_ij[:, None]\n",
    "\n",
    "      p_block = tl.exp(qk_block)\n",
    "\n",
    "      l_ij = tl.sum(p_block, 1)\n",
    "      alpha = tl.exp(m_i - m_ij)\n",
    "      l_i = alpha * l_i + l_ij\n",
    "\n",
    "      o_block *= alpha[:, None]\n",
    "      o_block += tl.dot(p_block, v_block, allow_tf32 = False)\n",
    "\n",
    "      m_i = m_ij\n",
    "\n",
    "    o_block /= l_i[:, None]\n",
    "    o_ofs = o_ptr + qo_ofs\n",
    "    tl.store(o_ofs, o_block, mask = qo_mask)\n",
    "\n",
    "\n",
    "def attention(q, k, v, head_qty, seq_len, head_size, softmax_scale):\n",
    "  block_size = 16\n",
    "  num_blocks = math.ceil(seq_len / block_size)\n",
    "  o = torch.zeros_like(q)\n",
    "  grid = (head_qty, num_blocks)\n",
    "  kernel[grid](q, k, v, o, head_qty, seq_len, head_size, softmax_scale, num_blocks, block_size)\n",
    "  return o\n",
    "\n",
    "head_qty = 4\n",
    "seq_len = 128\n",
    "head_size = 64\n",
    "\n",
    "softmax_scale = 1.0 / math.sqrt(head_size)\n",
    "q = torch.randn((head_qty, seq_len, head_size), device = 'cuda', dtype = torch.float32)\n",
    "k = torch.randn((head_qty, seq_len, head_size), device = 'cuda', dtype = torch.float32)\n",
    "v = torch.randn((head_qty, seq_len, head_size), device = 'cuda', dtype = torch.float32)\n",
    "\n",
    "output = attention(q, k, v, head_qty, seq_len, head_size, softmax_scale)\n",
    "\n",
    "compare = torch.nn.functional.scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "test = torch.allclose(output, compare, atol = 1e-6, rtol = 0)\n",
    "\n",
    "print(output)\n",
    "print(compare)\n",
    "\n",
    "if(test):\n",
    "  print(\"Test passed\")\n",
    "else:\n",
    "  print(\"Test failed\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
